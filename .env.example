# ==================================
# llmbattler Configuration
# ==================================
# Usage:
#   Development: cp .env.example .env
#   Production: Copy and modify for docker-compose.yml
#
# Note: Each service (backend/, worker/, frontend/) can have its own .env
#       to override these root settings if needed.

# ==================================
# Database
# ==================================
# Development (host machine)
POSTGRES_URI=postgresql+asyncpg://postgres:postgres@localhost:5432/llmbattler

# Production (Docker Compose - uncomment and modify)
# POSTGRES_USER=postgres
# POSTGRES_PASSWORD=your-secure-password-here
# POSTGRES_DB=llmbattler
# POSTGRES_URI=postgresql+asyncpg://postgres:your-secure-password-here@postgres:5432/llmbattler

# PostgreSQL connection pool
POSTGRES_POOL_SIZE=5
POSTGRES_MAX_OVERFLOW=10

# ==================================
# Backend API
# ==================================
# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
# Production example:
# CORS_ORIGINS=https://yourdomain.com,http://YOUR_SERVER_IP:3000

# Model configuration path (relative to backend directory)
MODELS_CONFIG_PATH=config/models.yaml
# Production (Docker) - use absolute path if needed:
# MODELS_CONFIG_PATH=/app/backend/config/models.yaml

# LLM API timeouts (seconds)
# Note: CPU inference can take 30-60s per request, so read timeout should be higher
LLM_CONNECT_TIMEOUT=5
LLM_READ_TIMEOUT=90
LLM_WRITE_TIMEOUT=5
LLM_POOL_TIMEOUT=5

# LLM API retry settings
LLM_RETRY_ATTEMPTS=3
LLM_RETRY_BACKOFF_BASE=1.0

# LLM Mock Mode (for development/testing without real LLM servers)
USE_MOCK_LLM=false

# Battle settings
MAX_FOLLOW_UPS=5
# Production:
# MAX_FOLLOW_UPS=10

# ==================================
# Frontend
# ==================================
NEXT_PUBLIC_API_URL=http://localhost:8000
# Production examples:
# NEXT_PUBLIC_API_URL=https://api.yourdomain.com
# NEXT_PUBLIC_API_URL=http://YOUR_SERVER_IP:8000

# ==================================
# Worker (Vote Aggregation)
# ==================================
# Run worker every N minutes (60 = 1 hour, 5 = 5 minutes for testing)
WORKER_INTERVAL_MINUTES=60
WORKER_TIMEZONE=UTC

# ==================================
# Leaderboard & ELO
# ==================================
MIN_VOTES_FOR_LEADERBOARD=5
INITIAL_ELO=1500
K_FACTOR=32

# ==================================
# Ollama (Self-hosted LLM)
# ==================================
# Models to download automatically on container startup (comma-separated)
# This is a custom variable for ollama-init.sh, NOT the official OLLAMA_MODELS env var
# Lightweight: gemma3:1b (for testing)
# Full set: tinyllama:1.1b,gemma2:2b,phi3:mini,qwen2.5:3b,mistral:7b,llama3.1:8b
OLLAMA_DOWNLOAD_MODELS=gemma3:1b

# GPU settings (0 for CPU, 1+ for GPU)
OLLAMA_GPU_ENABLED=0
OLLAMA_NUM_GPU=1

# ==================================
# External LLM API Keys (Optional)
# ==================================
# Only needed if you want to use external models
# For Ollama-only deployment, leave these empty

# OpenAI API Key (for GPT models)
# OPENAI_API_KEY=sk-...

# Anthropic API Key (for Claude models)
# ANTHROPIC_API_KEY=sk-ant-...

# ==================================
# Docker Resource Limits (Production)
# ==================================
# Uncomment for production deployment
# BACKEND_MEMORY_LIMIT=2g
# BACKEND_CPU_LIMIT=2.0
# WORKER_MEMORY_LIMIT=1g
# WORKER_CPU_LIMIT=1.0
# FRONTEND_MEMORY_LIMIT=1g
# FRONTEND_CPU_LIMIT=1.0
# OLLAMA_MEMORY_LIMIT=8g
# OLLAMA_CPU_LIMIT=4.0

# ==================================
# Logging (Production)
# ==================================
# LOG_LEVEL=INFO
