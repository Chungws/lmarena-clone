# ===========================================
# Production Environment Variables
# ===========================================
# Copy this file to .env.prod and fill in the values
# Usage: docker compose --env-file .env.prod up -d

# ===========================================
# Domain & URLs
# ===========================================
# For domain-based deployment:
#   DOMAIN=yourdomain.com
#   NEXT_PUBLIC_API_URL=https://api.yourdomain.com
#   FRONTEND_URL=https://yourdomain.com
#
# For IP-based deployment (replace YOUR_SERVER_IP with actual IP):
#   DOMAIN=YOUR_SERVER_IP
#   NEXT_PUBLIC_API_URL=http://YOUR_SERVER_IP:8000
#   FRONTEND_URL=http://YOUR_SERVER_IP:3000

DOMAIN=YOUR_SERVER_IP
NEXT_PUBLIC_API_URL=http://YOUR_SERVER_IP:8000
FRONTEND_URL=http://YOUR_SERVER_IP:3000

# ===========================================
# Database Configuration
# ===========================================
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your-secure-password-here
POSTGRES_DB=llmbattler
POSTGRES_URI=postgresql+asyncpg://postgres:your-secure-password-here@postgres:5432/llmbattler

# PostgreSQL connection pool
POSTGRES_POOL_SIZE=10
POSTGRES_MAX_OVERFLOW=20

# ===========================================
# LLM API Keys (External Services) - OPTIONAL
# ===========================================
# Only needed if you want to use external API models
# For Ollama-only deployment, you can leave these empty

# OpenAI API Key (for GPT models)
# OPENAI_API_KEY=sk-...

# Anthropic API Key (for Claude models)
# ANTHROPIC_API_KEY=sk-ant-...

# ===========================================
# Backend Configuration
# ===========================================
# For domain: https://yourdomain.com
# For IP: http://YOUR_SERVER_IP:3000
CORS_ORIGINS=http://YOUR_SERVER_IP:3000

# Model configuration
MODELS_CONFIG_PATH=/app/backend/config/models.yaml

# LLM API timeouts (seconds)
LLM_CONNECT_TIMEOUT=10
LLM_READ_TIMEOUT=60
LLM_WRITE_TIMEOUT=10
LLM_POOL_TIMEOUT=10

# LLM API retry settings
LLM_RETRY_ATTEMPTS=3
LLM_RETRY_BACKOFF_BASE=1.5

# Battle settings
MAX_FOLLOW_UPS=10

# Mock mode (set to false in production)
USE_MOCK_LLM=false

# ===========================================
# Worker Configuration
# ===========================================
WORKER_INTERVAL_HOURS=1
WORKER_TIMEZONE=UTC

# ELO settings
INITIAL_ELO=1500
K_FACTOR=32

# Leaderboard settings
MIN_VOTES_FOR_LEADERBOARD=5

# ===========================================
# Ollama Configuration
# ===========================================
# Models to download automatically (comma-separated)
# Lightweight set (1B-8B, no external API): tinyllama:1.1b,gemma2:2b,phi3:mini,qwen2.5:3b,mistral:7b,llama3.1:8b
# OLLAMA_MODELS=tinyllama:1.1b,gemma2:2b,phi3:mini,qwen2.5:3b,mistral:7b,llama3.1:8b
OLLAMA_MODELS=gemma3:1b

# GPU settings (set to 1 if you have GPU, 0 for CPU only)
OLLAMA_GPU_ENABLED=0
OLLAMA_NUM_GPU=1

# ===========================================
# Resource Limits (Optional)
# ===========================================
# Backend
BACKEND_MEMORY_LIMIT=2g
BACKEND_CPU_LIMIT=2.0

# Worker
WORKER_MEMORY_LIMIT=1g
WORKER_CPU_LIMIT=1.0

# Frontend
FRONTEND_MEMORY_LIMIT=1g
FRONTEND_CPU_LIMIT=1.0

# Ollama (adjust based on your GPU/CPU)
OLLAMA_MEMORY_LIMIT=8g
OLLAMA_CPU_LIMIT=4.0

# ===========================================
# Monitoring & Logging (Optional)
# ===========================================
LOG_LEVEL=INFO
