# llmbattler

**AI Language Model Battle Arena** - Compare and evaluate LLM responses through blind side-by-side testing.

## üéØ Project Purpose

llmbattler enables unbiased evaluation of Large Language Models (LLMs) by:
- Presenting anonymous side-by-side responses from two randomly selected models
- Collecting user votes to determine which response is better
- Building fair ELO-based leaderboards from real user preferences
- Supporting any OpenAI-compatible LLM endpoint (local or external)

## üöÄ Quick Start

See **[WORKSPACE/00_PROJECT.md](./WORKSPACE/00_PROJECT.md)** for:
- Complete setup instructions
- Development environment configuration
- Architecture overview
- Coding conventions

## üìö Documentation

| Document | Purpose |
|----------|---------|
| [WORKSPACE/00_PROJECT.md](./WORKSPACE/00_PROJECT.md) | Project overview, setup, policies |
| [WORKSPACE/00_ROADMAP.md](./WORKSPACE/00_ROADMAP.md) | Development roadmap and milestones |
| [WORKSPACE/CONVENTIONS/](./WORKSPACE/CONVENTIONS/) | Coding standards and guidelines |
| [WORKSPACE/FEATURES/](./WORKSPACE/FEATURES/) | Feature specifications |
| [CLAUDE.md](./CLAUDE.md) | AI assistant guidelines |

## üõ†Ô∏è Tech Stack

- **Frontend:** Next.js 15, React 19, shadcn/ui
- **Backend:** FastAPI, SQLModel, PostgreSQL, MongoDB
- **Worker:** Python 3.11+, APScheduler
- **AI:** Ollama, vLLM, OpenAI-compatible APIs

## üìù License

TBD

## ü§ù Contributing

This is a personal learning project. For questions or suggestions, please open an issue.
